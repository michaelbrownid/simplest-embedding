<header style="border-bottom: 1ps solid; background-color: #8c1515; text-align: center; font-size: 26; color:#fff; padding: 10;"><a style="color:#fff;" href="http://openboundlabs.com">openboundlabs.com</a></header>

<pre>

<main style="max-width:800px; padding: 30px;margin:0 auto;">

<h2>Goal</h2>
Simplest embedding problem using Tensorflow

Inspiried from this: <a href="http://matpalm.com/blog/2015/03/28/theano_word_embeddings/">http://matpalm.com/blog/2015/03/28/theano_word_embeddings/</a>

<img width=320px src="wiki-pic-major.png">
<a href="https://colah.github.io/posts/2015-01-Visualizing-Representations/">https://colah.github.io/posts/2015-01-Visualizing-Representations/</a>
<a href="https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/</a>

I first worked with embedding vectors in machine learning / neural
networks many years ago where we called them context vectors and used
them to do things like automatic text summarization, named entity
recognition, and semantic machine translation.

Embedding is powerful: <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a>, <a href="http://nlp.stanford.edu/projects/glove/">glove</a>, <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/">neural image captioning</a>, ...
Hinton started calling them <a href="https://deeplearning4j.org/thoughtvectors">thought vectors</a>.

The basic idea to to embed objects that might not have much direct
relation to each other into a space where higher semantic meaning
might be more easily seen using simple operations like distance. An
example is English words. Looking in the dictionary the words "happen"
and "happy" occur right next to each other. However their meanings are
quite different; alphabetic distance isn't too useful when dealing
with words. Wouldn't it be nice to embed those words in a space where
words with similar meanings are close together (viz: synonyms
happen==occur / happy==cheerful)....

================================

<h2>The Setup</h2>

- Predict whether two objects (o and p) are "compatible" / should occur together or not.

- Embed N arbitrary objects identified by integers into 2d (x,y) (so I can plot them)

- Try learning a simple linear ordering "compatibility" function:

  - Make adjacent pairs compatible Prob(o,p)=1.0 : (o,p) = (0,1), (1,2), (4,5), ...
  
  - Make all others incompatible Prob(o,p)=0.0 : (o,p) = (0,2), (3,5), ...

  - Assume pairs are ordered so (o,p) -> p>o

- Try a very simple learning network:

  Input: (o, p) (object integers)

  Embed: embedding(o) = 2d

  Computation: f(o,p) = single linear layer.
	       input = [embedding(x), embedding(y)]
	       output= logit for [1-P, P] where P is probability o,p are compatible

  Output: softmax of network output

- The key here is that when learning, gradients can be pushed back to
  the embedding vectors so they can be updated. It is _automatic_ in
  frameworks like Tensorflow.

Source Code: <a href="https://github.com/michaelbrownid/simplest-embedding">https://github.com/michaelbrownid/simplest-embedding</a> or simply <a href="simplest-embedding.py">simplest-embedding.py</a>

Note this is interesting because we will estimate both the embedding
and the linear discriminant function on the embedding simultaneously.

TODO: eliminate the linear function and use dot product with no
learned parameters.

================================

<h2>RESULT</h2>
Animate the embeddings and the decision boundries for the first 4
points (colors=black, red, green, blue) every 3 epochs of optimization
for 1000 epochs total.


<div style="max-width:600;"><img id="i1" src="simpleimg1/simplest-embedding-120.png"></div>
<div style="display: none;" id="i2"><img src="simpleimg1/simplest-embedding-120.png"></div>

<div style="font-size:60%; border: 1px solid; background-color: rgba(1,1,1,0.2);">
<h2>What experiment do you want to see?</h2>
<input type="radio" name="exp" value="simpleimg1" checked="checked">Coverged1</br>
<input type="radio" name="exp" value="simpleimg2">NotQuite</br>
<input type="radio" name="exp" value="simpleimg3">Converged2</br>
current <input id="curr" type="number" value="40"> / 333
<button type="button" id="prev">prev</button> <button type="button" id="next">next</button><br>
frame step <input id="step" type="number" value="1"><br>
frame delay <input id="time" type="number" value="1000"> Smaller for faster animation!
<h2>Animate!</h2><button style="width: 256px; height: 64px;" type="button" id="start">animate start/stop</button> <br>
</div>


================================

The network can mostly learn the simple linear ordering! (A couple of
errors when not converged fully)

This isn't too exciting but it is interesting to see the nice linear
chain that is learned from a number of pairwise examples. It reminds
me of the Hammersley-Clifford theorem where a joint density can be
factorized over cliques (<a href="https://en.wikipedia.org/wiki/Hammersley%E2%80%93Clifford_theorem">Hammersley-Clifford theorem</a>)

<img width=128 src="Markov_random_field_example.png">

It seems the ordering happens fairly quickly then the decision boundry
tightens up.

It is simple but interesting. Next it might be interesting to take ZIP
codes and embed them. First simply try to "learn"
ZIP->(latitude,longitude). Then try to pull out additional covariates
like median income, total population, ..., from the embedded vectors.

================================
</main>
</pre>

<footer style="border-top: 1px solid; padding: 30px 0;">
  <a href="http://openboundlabs.com">openboundlabs.com</a> _
  <a href="https://github.com/michaelbrownid">
    <span class="icon github">
      <svg style="height:24;" version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
      </svg>
    </span> _</a>
    <a href="https://twitter.com/michaelbrownid">
    <span class="icon twitter">
      <svg style="height:24;" version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
      </svg>
    </span> _</a>
  <a href="mailto:michaelbrownid@openboundlabs.com">michaelbrownid@openboundlabs.com</a> _
  Michael P.S. Brown _ Fri Dec 23 11:16:44 PST 2016
</footer>

<script>

imglist = function(num){
  myexp = document.querySelector('input[name="exp"]:checked').value;
  return(""+myexp+"/simplest-embedding-"+3*num+".png")
}

var current=40;
var myinterval = null;
var mysrc;
var s;
var img1;


updateimg = function() {
  mysrc = imglist(current)
  document.getElementById("i1").src=mysrc
  //document.getElementById("i1").width=500
  document.getElementById("curr").value = ""+current

  mysrc = "<img src='" + imglist(nextval()) + "'>"
  document.getElementById("i2").innerHTML=mysrc

}

prevval = function(){
  tmp = current -parseInt(step.value)
  if (tmp<0){ tmp = 333; }
  return(tmp)
}
nextval = function(){
  tmp = current +parseInt(step.value)
  if (tmp>333){ tmp = 0; }
  return(tmp)
}

hitprev = function(){
  current = prevval()
  updateimg()
}
hitnext = function(){
  current = nextval()
  updateimg()
}


document.getElementById("prev").onclick = hitprev;
document.getElementById("next").onclick = hitnext;

document.getElementById("start").onclick = function() {
  if (myinterval==null){
    myinterval = setInterval(hitnext,parseInt(time.value))
  } else {
    clearInterval(myinterval)
    myinterval=null
  }
}

document.getElementById("curr").onchange = function() {
  current = parseInt(curr.value);
  <!-- #id-or-getelement: current = parseInt(document.getElementById("curr").value); -->
  updateimg();
}

// start the animation here at the end
// myinterval = setInterval(hitnext,parseInt(time.value))

</script>
